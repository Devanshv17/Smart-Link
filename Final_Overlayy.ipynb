{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Q3PYiY3iqTv5",
        "CyYLpUrxBpk9",
        "G8PeR7zVrgEI",
        "W43XxpQ--lwv",
        "hX01tTldOq3F"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dependencies\n"
      ],
      "metadata": {
        "id": "Q3PYiY3iqTv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T4r3trsXqSxY",
        "outputId": "fba1d584-703c-4494-89a7-2a21ea13a241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-embeddings-gemini\n",
            "  Downloading llama_index_embeddings_gemini-0.2.0-py3-none-any.whl.metadata (704 bytes)\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-core\n",
            "  Downloading llama_index_core-0.11.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-readers-web\n",
            "  Downloading llama_index_readers_web-0.2.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting google-generativeai<0.6.0,>=0.5.2 (from llama-index-embeddings-gemini)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.0-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.3-py3-none-any.whl.metadata (635 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.0-py3-none-any.whl.metadata (648 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting nltk>3.8.1 (from llama-index)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (3.10.5)\n",
            "Collecting dataclasses-json (from llama-index-core)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2024.6.1)\n",
            "Collecting httpx (from llama-index-core)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (9.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-web) (4.12.3)\n",
            "Collecting chromedriver-autoinstaller<0.7.0,>=0.6.3 (from llama-index-readers-web)\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting html2text<2025.0.0,>=2024.2.26 (from llama-index-readers-web)\n",
            "  Downloading html2text-2024.2.26.tar.gz (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting newspaper3k<0.3.0,>=0.2.8 (from llama-index-readers-web)\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting playwright<2.0,>=1.30 (from llama-index-readers-web)\n",
            "  Downloading playwright-1.46.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting selenium<5.0.0,>=4.17.2 (from llama-index-readers-web)\n",
            "  Downloading selenium-4.23.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting spider-client<0.0.28,>=0.0.27 (from llama-index-readers-web)\n",
            "  Downloading spider-client-0.0.27.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-web) (2.0.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-web) (2.6)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver-autoinstaller<0.7.0,>=0.6.3->llama-index-readers-web) (24.1)\n",
            "Collecting google-ai-generativelanguage==0.6.4 (from google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini)\n",
            "  Downloading google_ai_generativelanguage-0.6.4-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (1.24.0)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.0.15-py3-none-any.whl.metadata (751 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.1.4)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.2.0->llama-index)\n",
            "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (4.9.4)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: greenlet==3.0.3 in /usr/local/lib/python3.10/dist-packages (from playwright<2.0,>=1.30->llama-index-readers-web) (3.0.3)\n",
            "Collecting pyee==11.1.0 (from playwright<2.0,>=1.30->llama-index-readers-web)\n",
            "  Downloading pyee-11.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (2024.7.4)\n",
            "Collecting trio~=0.17 (from selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium<5.0.0,>=4.17.2->llama-index-readers-web) (1.8.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (1.63.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (4.9)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core) (1.2.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (3.15.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.17.2->llama-index-readers-web) (1.7.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-embeddings-gemini) (0.6.0)\n",
            "Downloading llama_index_embeddings_gemini-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index-0.11.1-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_core-0.11.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_web-0.2.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading google_generativeai-0.5.4-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.6.4-py3-none-any.whl (679 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.2.3-py3-none-any.whl (6.3 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.2.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading playwright-1.46.0-py3-none-manylinux1_x86_64.whl (37.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-11.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading selenium-4.23.1-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_cloud-0.0.15-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: html2text, tinysegmenter, spider-client, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html2text: filename=html2text-2024.2.26-py3-none-any.whl size=33111 sha256=73ff1ca0ad5efae8c611fcda268466ade2254285ea70f18051ab6cc9e5250c18\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/96/6d/a7eba8f80d31cbd188a2787b81514d82fc5ae6943c44777659\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=edf690eb119e8b284c2d7f3990af58f427b967ad7aa0c6b319a8ccfddbca6052\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spider-client: filename=spider_client-0.0.27-py3-none-any.whl size=5977 sha256=df93fb73a5e570018f38a39516aa4c587f5e89a4248323dc488d545a1cd918e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/e0/39/c07e639cb4ea75f06d4574ec3052b64d66ae8cd355a9d3a570\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=ac973f596466a65080ef68e5c65a64e3196e7b01af264abf07814ae7b6096f6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=6b22b42038037e5618a2981a60af9bc5425aee3cc3fa9351f00cbfcd19424f7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=17622a8492eae21d05b5b7b1ea536fc37accf1a7268b47f3a0c3948f88dced2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built html2text tinysegmenter spider-client feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, striprtf, sgmllib3k, jieba3k, dirtyjson, tenacity, pypdf, pyee, outcome, nltk, mypy-extensions, marshmallow, jiter, html2text, h11, feedparser, deprecated, cssselect, chromedriver-autoinstaller, wsproto, typing-inspect, trio, tiktoken, spider-client, requests-file, playwright, httpcore, feedfinder2, trio-websocket, tldextract, httpx, dataclasses-json, selenium, openai, newspaper3k, llama-index-core, llama-cloud, llama-parse, llama-index-readers-web, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, google-ai-generativelanguage, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, google-generativeai, llama-index-program-openai, llama-index-embeddings-gemini, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.6\n",
            "    Uninstalling google-ai-generativelanguage-0.6.6:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.6\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.7.2\n",
            "    Uninstalling google-generativeai-0.7.2:\n",
            "      Successfully uninstalled google-generativeai-0.7.2\n",
            "Successfully installed chromedriver-autoinstaller-0.6.4 cssselect-1.2.0 dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 feedfinder2-0.0.4 feedparser-6.0.11 google-ai-generativelanguage-0.6.4 google-generativeai-0.5.4 h11-0.14.0 html2text-2024.2.26 httpcore-1.0.5 httpx-0.27.0 jieba3k-0.35.1 jiter-0.5.0 llama-cloud-0.0.15 llama-index-0.11.1 llama-index-agent-openai-0.3.0 llama-index-cli-0.3.0 llama-index-core-0.11.1 llama-index-embeddings-gemini-0.2.0 llama-index-embeddings-openai-0.2.3 llama-index-indices-managed-llama-cloud-0.3.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.0 llama-index-multi-modal-llms-openai-0.2.0 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.0 llama-index-readers-llama-parse-0.2.0 llama-index-readers-web-0.2.1 llama-parse-0.5.0 marshmallow-3.22.0 mypy-extensions-1.0.0 newspaper3k-0.2.8 nltk-3.9.1 openai-1.42.0 outcome-1.3.0.post0 playwright-1.46.0 pyee-11.1.0 pypdf-4.3.1 requests-file-2.1.0 selenium-4.23.1 sgmllib3k-1.0.0 spider-client-0.0.27 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.7.0 tinysegmenter-0.3 tldextract-5.1.2 trio-0.26.2 trio-websocket-0.11.1 typing-inspect-0.9.0 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "d2d25c8d38c34d4ca50e0c328c49c31e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/4.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/4.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m4.0/4.5 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (2024.7.4)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (8.1.7)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting curl-cffi\n",
            "  Downloading curl_cffi-0.7.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-6.2.11-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage in /usr/local/lib/python3.10/dist-packages (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.10/dist-packages (1.63.2)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (1.64.1)\n",
            "Requirement already satisfied: grpcio-status in /usr/local/lib/python3.10/dist-packages (1.48.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (3.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: proto-plus in /usr/local/lib/python3.10/dist-packages (1.24.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (2.22)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: pydantic-core in /usr/local/lib/python3.10/dist-packages (2.20.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: rsa in /usr/local/lib/python3.10/dist-packages (4.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Collecting primp>=0.6.1 (from duckduckgo-search)\n",
            "  Downloading primp-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading curl_cffi-0.7.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading duckduckgo_search-6.2.11-py3-none-any.whl (27 kB)\n",
            "Downloading primp-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, colorama, duckduckgo-search, curl-cffi\n",
            "Successfully installed colorama-0.4.6 curl-cffi-0.7.1 duckduckgo-search-6.2.11 primp-0.6.1\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [945 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,556 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,234 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,425 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [53.3 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,498 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\n",
            "Fetched 16.2 MB in 2s (7,263 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 118 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04ubuntu0.1 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.5 MB in 3s (9,530 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123803 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.63+22.04ubuntu0.1_amd64.deb ...\n",
            "Unpacking snapd (2.63+22.04ubuntu0.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.63+22.04ubuntu0.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124033 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.23.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.23.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-embeddings-gemini llama-index llama-index-core llama-index-readers-web\n",
        "!pip install -q llama-index google-generativeai matplotlib llama-index-embeddings-gemini llama-index-llms-gemini\n",
        "!pip install annotated-types cachetools certifi cffi charset-normalizer click colorama curl-cffi duckduckgo-search google-ai-generativelanguage google-api-core google-auth google-generativeai googleapis-common-protos grpcio grpcio-status idna lxml proto-plus protobuf pyasn1 pyasn1-modules pycparser pydantic pydantic-core requests rsa tqdm typing-extensions urllib3\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install selenium beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Enter the desired website Url"
      ],
      "metadata": {
        "id": "CyYLpUrxBpk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global website_url\n",
        "website_url = \"\"\n",
        "website_url = input(\"Enter the Website link: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hztFstnXJnei",
        "outputId": "e9f46c3c-aba3-43e7-a1e2-4bb163b45297"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the Website link: https://geeksforgeeks.org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrapper"
      ],
      "metadata": {
        "id": "G8PeR7zVrgEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import ElementClickInterceptedException, TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Set up Chrome options with a custom user-agent\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\")\n",
        "\n",
        "def initialize_driver():\n",
        "    \"\"\"Initialize WebDriver with custom options.\"\"\"\n",
        "    return webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "def open_website(driver, url):\n",
        "    \"\"\"Open the target website and wait for it to load.\"\"\"\n",
        "    driver.get(url)\n",
        "    WebDriverWait(driver, 20).until(\n",
        "        EC.presence_of_element_located((By.TAG_NAME, 'body'))\n",
        "    )\n",
        "    time.sleep(5)  # Add a sleep to ensure all dynamic content is loaded\n",
        "\n",
        "def extract_links(driver):\n",
        "    \"\"\"Extract all href links from the page and return them in a set.\"\"\"\n",
        "    page_source = driver.page_source\n",
        "    soup = BeautifulSoup(page_source, 'html.parser')\n",
        "    links = soup.find_all('a', href=True)\n",
        "    print(f\"Found {len(links)} links on the page.\")  # Debugging line\n",
        "    hrefs = {link['href'] for link in links if link['href'].startswith(('http', 'https'))}\n",
        "    print(f\"Extracted {len(hrefs)} unique hrefs.\")  # Debugging line\n",
        "    return hrefs\n",
        "\n",
        "def extract_href_from_element(element):\n",
        "    \"\"\"Extract href attribute from the element if available.\"\"\"\n",
        "    try:\n",
        "        if element.tag_name == 'a':\n",
        "            return element.get_attribute('href')\n",
        "        elif element.tag_name == 'button':\n",
        "            # Check if the button is a link or has a hidden link\n",
        "            a_tags = element.find_elements(By.XPATH, \".//a\")\n",
        "            if a_tags:\n",
        "                return a_tags[0].get_attribute('href')\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting href from element: {e}\")\n",
        "    return None\n",
        "\n",
        "def process_clickable_elements(driver, elements, hrefs):\n",
        "    \"\"\"Process each clickable element and add its href to the set.\"\"\"\n",
        "    for i, element in enumerate(elements):\n",
        "        try:\n",
        "            href = extract_href_from_element(element)\n",
        "            if href:\n",
        "                hrefs.add(href)\n",
        "        except (StaleElementReferenceException, NoSuchElementException) as e:\n",
        "            print(f\"Error interacting with element: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "\n",
        "def handle_hover_elements(driver, hrefs):\n",
        "    \"\"\"Handle elements that require hovering to reveal hidden links.\"\"\"\n",
        "    hover_elements = driver.find_elements(By.CSS_SELECTOR, \"css-selector-for-hover-elements\")  # Replace with actual CSS selector\n",
        "    if not hover_elements:\n",
        "        hover_elements = driver.find_elements(By.XPATH, \"xpath-for-hover-elements\")  # Replace with actual XPath\n",
        "\n",
        "    actions = ActionChains(driver)\n",
        "    for element in hover_elements:\n",
        "        try:\n",
        "            actions.move_to_element(element).perform()\n",
        "            time.sleep(2)  # Wait for the hover-triggered content to appear\n",
        "\n",
        "            # Re-extract the links after hovering\n",
        "            page_source_after_hover = driver.page_source\n",
        "            soup_after_hover = BeautifulSoup(page_source_after_hover, 'html.parser')\n",
        "            hover_links = soup_after_hover.find_all('a', href=True)\n",
        "\n",
        "            for link in hover_links:\n",
        "                href = link['href']\n",
        "                if href.startswith(('http', 'https')):\n",
        "                    hrefs.add(href)\n",
        "        except Exception as e:\n",
        "            print(f\"Error hovering over element: {e}\")\n",
        "\n",
        "def remove_unwanted_links(links, website_url):\n",
        "    \"\"\"Remove specific links from the set if website_url does not start with certain URLs.\"\"\"\n",
        "    unwanted_prefixes = [\n",
        "        \"https://x.com\",\n",
        "        \"https://www.youtube\",\n",
        "        \"https://www.linkedin\",\n",
        "        \"https://in.linkedin.com\",\n",
        "        \"https://github\",\n",
        "        \"https://www.instagram\",\n",
        "        \"https://www.tiktok.com\",\n",
        "        \"https://www.facebook\",\n",
        "        \"https://www.twitter\",\n",
        "        \"https://www.reddit\",\n",
        "        \"https://discord\"\n",
        "    ]\n",
        "\n",
        "    if not any(website_url.startswith(prefix) for prefix in unwanted_prefixes):\n",
        "        links = {link for link in links if not any(link.startswith(prefix) for prefix in unwanted_prefixes)}\n",
        "\n",
        "    return links\n",
        "\n",
        "def save_links_to_json(links, filename):\n",
        "    \"\"\"Save the set of links to a JSON file.\"\"\"\n",
        "    data = {'links': list(links)}  # Create a dictionary with the key 'links'\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "    print(f\"Links saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    driver = initialize_driver()\n",
        "    hrefs = set()\n",
        "\n",
        "    try:\n",
        "        open_website(driver, website_url)\n",
        "\n",
        "        # Extract initial links\n",
        "        hrefs.update(extract_links(driver))\n",
        "\n",
        "        # Find and click a button (replace with actual button XPath if needed)\n",
        "        try:\n",
        "            button_xpath = 'your_button_xpath_here'  # Replace with the correct XPath\n",
        "            button = WebDriverWait(driver, 10).until(\n",
        "                EC.element_to_be_clickable((By.XPATH, button_xpath))\n",
        "            )\n",
        "\n",
        "            # Scroll into view\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n",
        "\n",
        "            # Attempt to click the button with exception handling\n",
        "            try:\n",
        "                button.click()\n",
        "            except ElementClickInterceptedException:\n",
        "                driver.execute_script(\"arguments[0].click();\", button)\n",
        "            time.sleep(2)  # Wait for the content to load\n",
        "\n",
        "        except TimeoutException:\n",
        "            print(\"Button not found or could not be clicked.\")\n",
        "        except Exception as e:\n",
        "            print(\"Error clicking button: \", e)\n",
        "\n",
        "        # Find all clickable elements\n",
        "        clickable_elements = driver.find_elements(By.XPATH, \"//button | //a\")\n",
        "        if clickable_elements:\n",
        "            process_clickable_elements(driver, clickable_elements, hrefs)\n",
        "\n",
        "        # Handle hover elements if needed\n",
        "        handle_hover_elements(driver, hrefs)\n",
        "\n",
        "        # Remove unwanted links\n",
        "        hrefs = remove_unwanted_links(hrefs, website_url)\n",
        "\n",
        "        # Save links to JSON\n",
        "        save_links_to_json(hrefs, 'links.json')\n",
        "\n",
        "    except (TimeoutException, NoSuchElementException) as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Close the WebDriver\n",
        "        driver.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygXUYvQ3rqzN",
        "outputId": "3ebefe24-a35f-4f8f-fa42-ebd9b2ce8016"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 289 links on the page.\n",
            "Extracted 278 unique hrefs.\n",
            "Button not found or could not be clicked.\n",
            "Links saved to links.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question Generation"
      ],
      "metadata": {
        "id": "iIZTQB9l7QIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from duckduckgo_search import DDGS\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Configure the Google Generative AI API key\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize DuckDuckGo search with a timeout of 20 seconds\n",
        "duckduckgo = DDGS(timeout=20)\n",
        "\n",
        "# Configuration for the model's output\n",
        "generation_config = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 2048,\n",
        "}\n",
        "\n",
        "# Safety settings for the model\n",
        "safety_settings = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "]\n",
        "\n",
        "# Initialize the generative model\n",
        "model = genai.GenerativeModel(\n",
        "    \"gemini-1.5-pro\",\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        ")\n",
        "\n",
        "# Define the search query template\n",
        "search_template = \"\"\"Convert the following to a search engine query: {question}\n",
        "Search query:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "\n",
        "# Generate the search query using the model\n",
        "search_query = model.generate_content(search_template.format(question=f\"Detailed Information about the follwing website:- {website_url}\")).text\n",
        "print(\"-- LLM Generated search query:\", search_query)\n",
        "\n",
        "# Perform a search using DuckDuckGo\n",
        "try:\n",
        "    search_result = duckduckgo.text(search_query, max_results=7)\n",
        "except Exception as e:\n",
        "    print(\"Error during search:\", e)\n",
        "    search_result = []  # Set an empty result in case of error\n",
        "\n",
        "time.sleep(1)\n",
        "\n",
        "# Format the search results\n",
        "search_results = \"\\n\\n\".join(\n",
        "    [\n",
        "        \"{_index}. {_title}\\n{_body}\\nSource: {_href}\".format(\n",
        "            _index=i + 1,\n",
        "            _title=result[\"title\"],\n",
        "            _body=result[\"body\"],\n",
        "            _href=result[\"href\"],\n",
        "        )\n",
        "        for i, result in enumerate(search_result)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the template for the final response\n",
        "response_template = \"\"\"\n",
        "You are a helpful search assistant. Be nice, talkative, and informative.\n",
        "Answer the user's question based on the search results below:\n",
        "Search results:\n",
        "{search_results}\n",
        "User question:\n",
        "{question}\n",
        "Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "# Generate the final response using the model\n",
        "response = model.generate_content(\n",
        "    response_template.format(search_results=search_results, question=f\"Give me a summary of the functionalities and usage purpose of the following website '{website_url}' in more than 200 words\")\n",
        ")\n",
        "\n",
        "# Output the final response\n",
        "website_info = response.text\n",
        "print(\"Website Information:\\n\", website_info)\n",
        "\n",
        "# Define the template for generating questions\n",
        "questions_template = \"\"\"\n",
        "Generate 10 questions based on the following information about a website.\n",
        "Ensure each question is no more than 70 characters long.\n",
        "Information:\n",
        "{website_info}\n",
        "Questions:\n",
        "\"\"\".strip()\n",
        "\n",
        "# Generate questions based on the website information\n",
        "questions_response = model.generate_content(\n",
        "    questions_template.format(website_info=website_info)\n",
        ")\n",
        "\n",
        "# Output the generated questions\n",
        "questions_text = questions_response.text\n",
        "print(\"Generated Questions:\\n\", questions_text)\n",
        "\n",
        "# Save the questions to a JSON file\n",
        "questions_list = questions_text.split('\\n')\n",
        "questions_data = {\"questions\": questions_list}\n",
        "\n",
        "with open('generated_questions.json', 'w') as json_file:\n",
        "    json.dump(questions_data, json_file, indent=4)\n",
        "\n",
        "print(\"Questions saved to 'generated_questions.json'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "YtFKhkpD747f",
        "outputId": "cac8c89d-4393-41be-9c74-139d4e57cc93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- LLM Generated search query: \"geeksforgeeks.org\" site analysis -site:geeksforgeeks.org \n",
            "\n",
            "Website Information:\n",
            " GeeksforGeeks (geeksforgeeks.org) is a popular online platform that serves as a comprehensive resource for programming enthusiasts and computer science students. The website offers a wealth of information, tutorials, and coding examples covering a wide range of topics, including algorithms, data structures, programming languages, interview preparation, and more. \n",
            "\n",
            "One of the primary functionalities of GeeksforGeeks is its extensive library of articles and tutorials. These resources are carefully curated and written in a clear and concise manner, making it easy for learners to grasp complex concepts. The website also features a vast collection of coding examples in multiple programming languages, allowing users to see practical implementations of theoretical concepts. \n",
            "\n",
            "Furthermore, GeeksforGeeks is highly regarded for its interview preparation materials. The platform provides a dedicated section with interview experiences, frequently asked questions, and company-specific preparation guides. This makes it an invaluable resource for individuals preparing for technical interviews at top tech companies.\n",
            "\n",
            "The website's popularity and usefulness are evident in its impressive traffic statistics. According to ranking websites, GeeksforGeeks consistently ranks among the top websites globally and attracts millions of visitors each month. Its user base primarily consists of individuals from India, where it holds the top ranking in its category. \n",
            "\n",
            "Overall, GeeksforGeeks has established itself as a go-to destination for anyone seeking to enhance their programming skills, prepare for technical interviews, or simply expand their knowledge in computer science. Its comprehensive content, user-friendly interface, and focus on practical application make it an invaluable resource for learners of all levels. \n",
            "\n",
            "Generated Questions:\n",
            " Here are 10 questions based on the text, each under 70 characters:\n",
            "\n",
            "1. What is GeeksforGeeks known for?\n",
            "2. Who benefits most from GeeksforGeeks?\n",
            "3. How does GeeksforGeeks aid interview prep?\n",
            "4. What type of content is on GeeksforGeeks?\n",
            "5. Why are GeeksforGeeks' tutorials popular?\n",
            "6. What programming languages are covered?\n",
            "7. How does GeeksforGeeks rank globally?\n",
            "8. Where is GeeksforGeeks most popular?\n",
            "9. What makes GeeksforGeeks user-friendly?\n",
            "10. Is GeeksforGeeks suitable for beginners? \n",
            "\n",
            "Questions saved to 'generated_questions.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question Matching and sorting the top 5 as per the requirements"
      ],
      "metadata": {
        "id": "W43XxpQ--lwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "\n",
        "# Load questions and links from JSON files\n",
        "with open('/content/generated_questions.json') as q_file:\n",
        "    questions_data = json.load(q_file)\n",
        "\n",
        "with open('/content/links.json') as l_file:\n",
        "    links_data = json.load(l_file)\n",
        "\n",
        "questions = questions_data['questions']\n",
        "links = links_data['links']  # Correctly referring to the list of links\n",
        "\n",
        "# Function to extract text content from a URL\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return soup.get_text()\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "# Create a list of documents to compare (questions + extracted link content)\n",
        "documents = questions + [extract_text_from_url(link) for link in links]\n",
        "\n",
        "# Calculate TF-IDF vectors for all documents\n",
        "vectorizer = TfidfVectorizer().fit_transform(documents)\n",
        "vectors = vectorizer.toarray()\n",
        "\n",
        "# Calculate cosine similarity between each question and link content\n",
        "similarity_matrix = cosine_similarity(vectors[:len(questions)], vectors[len(questions):])\n",
        "\n",
        "# Sum up the similarity scores for each link across all questions\n",
        "link_scores = np.sum(similarity_matrix, axis=0)\n",
        "\n",
        "# Get the indices of the top 5 links with the highest overall scores\n",
        "top_link_indices = np.argsort(link_scores)[-5:][::-1]\n",
        "\n",
        "# Select the top 5 links based on the overall relevance\n",
        "top_links = [links[i] for i in top_link_indices]\n",
        "\n",
        "# Create the output JSON with the top 5 most relevant links\n",
        "output = {\"top_links\": top_links}\n",
        "\n",
        "# Save the output JSON\n",
        "with open('top_links.json', 'w') as outfile:\n",
        "    json.dump(output, outfile, indent=4)\n",
        "\n",
        "print(\"Top 5 links related to most of the questions saved to 'top_links.json'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwi8Djeb-dPo",
        "outputId": "d91a26ff-3f67-4e2a-ee02-680e610da9b1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 links related to most of the questions saved to 'top_links.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Title of the top 5"
      ],
      "metadata": {
        "id": "bQl5SGYMMZdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Load the top links from the JSON file\n",
        "with open('/content/top_links.json') as infile:\n",
        "    top_links_data = json.load(infile)\n",
        "\n",
        "top_links = top_links_data[\"top_links\"]\n",
        "\n",
        "# Function to extract the title of a webpage\n",
        "def extract_title_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return soup.title.string.strip() if soup.title else \"No Title Found\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching title: {str(e)}\"\n",
        "\n",
        "# Create a new list with titles and corresponding links\n",
        "updated_links_with_titles = [{\"url\": link, \"title\": extract_title_from_url(link)} for link in top_links]\n",
        "\n",
        "# Update the JSON structure\n",
        "output_with_titles = {\"relevant_links\": updated_links_with_titles}\n",
        "\n",
        "# Save the updated JSON with titles\n",
        "with open('relevant_links.json', 'w') as outfile:\n",
        "    json.dump(output_with_titles, outfile, indent=4)\n",
        "\n",
        "print(\"Top 5 links with titles saved to 'relevant_links.json'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31kCAPlFMefP",
        "outputId": "8629f4f2-650a-42f3-9005-6b140fcf27d0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 links with titles saved to 'relevant_links.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Output Generation"
      ],
      "metadata": {
        "id": "hX01tTldOq3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load generated_questions.json\n",
        "with open('generated_questions.json', 'r') as file:\n",
        "    generated_questions = json.load(file)\n",
        "\n",
        "# Load relevant_links.json\n",
        "with open('relevant_links.json', 'r') as file:\n",
        "    relevant_links = json.load(file)\n",
        "\n",
        "# Define the base URL for the final object\n",
        "url = \"https://www.geeksforgeeks.org\"\n",
        "\n",
        "# Remove empty strings from the questions list\n",
        "questions = [q for q in generated_questions[\"questions\"] if q.strip()]\n",
        "\n",
        "# Construct the final JSON object\n",
        "final_output = {\n",
        "    \"url\": website_url,\n",
        "    \"questions\": questions,\n",
        "    \"relevant_links\": relevant_links[\"relevant_links\"]\n",
        "}\n",
        "\n",
        "# Print or save the final output\n",
        "print(json.dumps(final_output, indent=4))\n",
        "\n",
        "# Optionally, save to a file\n",
        "with open('output.json', 'w') as file:\n",
        "    json.dump(final_output, file, indent=4)\n",
        "\n",
        "print(\"Data has been saved to output.json.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqeRk1a8OrY6",
        "outputId": "afd58993-6da6-4a46-dca1-10172544abf0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"url\": \"https://geeksforgeeks.org\",\n",
            "    \"questions\": [\n",
            "        \"Here are 10 questions based on the text, each under 70 characters:\",\n",
            "        \"1. What is GeeksforGeeks known for?\",\n",
            "        \"2. Who benefits most from GeeksforGeeks?\",\n",
            "        \"3. How does GeeksforGeeks aid interview prep?\",\n",
            "        \"4. What type of content is on GeeksforGeeks?\",\n",
            "        \"5. Why are GeeksforGeeks' tutorials popular?\",\n",
            "        \"6. What programming languages are covered?\",\n",
            "        \"7. How does GeeksforGeeks rank globally?\",\n",
            "        \"8. Where is GeeksforGeeks most popular?\",\n",
            "        \"9. What makes GeeksforGeeks user-friendly?\",\n",
            "        \"10. Is GeeksforGeeks suitable for beginners? \"\n",
            "    ],\n",
            "    \"relevant_links\": [\n",
            "        {\n",
            "            \"url\": \"https://www.geeksforgeeks.org/programming-languages/?ref=home_viewall_row4\",\n",
            "            \"title\": \"Learn Programming Languages- List of 11 Popular Programming Languages - GeeksforGeeks\"\n",
            "        },\n",
            "        {\n",
            "            \"url\": \"https://www.geeksforgeeks.org/python-interview-questions/?ref=outindfooter\",\n",
            "            \"title\": \"Top 50+ Python Interview Questions and Answers (Latest 2024)\"\n",
            "        },\n",
            "        {\n",
            "            \"url\": \"https://www.geeksforgeeks.org/introduction-to-programming-languages/?ref=outindfooter\",\n",
            "            \"title\": \"Introduction to Programming Languages - GeeksforGeeks\"\n",
            "        },\n",
            "        {\n",
            "            \"url\": \"https://www.geeksforgeeks.org/geeksforgeeks-school/?ref=home_viewall_row10\",\n",
            "            \"title\": \"GeeksforGeeks School - GeeksforGeeks\"\n",
            "        },\n",
            "        {\n",
            "            \"url\": \"https://www.geeksforgeeks.org/most-commonly-asked-system-design-interview-problems-questions/?ref=outindfooter\",\n",
            "            \"title\": \"Most Commonly Asked System Design Interview Problems/Questions - GeeksforGeeks\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "Data has been saved to output.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metric\n"
      ],
      "metadata": {
        "id": "FZl46NS6P83u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from duckduckgo_search import DDGS\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Configure the Google Generative AI API key\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize DuckDuckGo search with a timeout of 20 seconds\n",
        "duckduckgo = DDGS(timeout=20)\n",
        "\n",
        "# Configuration for the model's output\n",
        "generation_config = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 2048,\n",
        "}\n",
        "\n",
        "# Safety settings for the model\n",
        "safety_settings = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "]\n",
        "\n",
        "# Initialize the generative model\n",
        "model = genai.GenerativeModel(\n",
        "    \"gemini-1.5-pro\",\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        ")\n",
        "\n",
        "# Define the search query template\n",
        "search_template = \"\"\"Convert the following to a search engine query: {question}\n",
        "Search query:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "\n",
        "# Generate the search query using the model\n",
        "search_query = model.generate_content(search_template.format(question=f\"Detailed Information about the follwing website:- {website_url}\")).text\n",
        "\n",
        "\n",
        "# Perform a search using DuckDuckGo\n",
        "try:\n",
        "    search_result = duckduckgo.text(search_query, max_results=7)\n",
        "except Exception as e:\n",
        "    print(\"Error during search:\", e)\n",
        "    search_result = []  # Set an empty result in case of error\n",
        "\n",
        "time.sleep(1)\n",
        "\n",
        "# Format the search results\n",
        "search_results = \"\\n\\n\".join(\n",
        "    [\n",
        "        \"{_index}. {_title}\\n{_body}\\nSource: {_href}\".format(\n",
        "            _index=i + 1,\n",
        "            _title=result[\"title\"],\n",
        "            _body=result[\"body\"],\n",
        "            _href=result[\"href\"],\n",
        "        )\n",
        "        for i, result in enumerate(search_result)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the template for the final response\n",
        "response_template = \"\"\"\n",
        "You are a helpful search assistant. Be nice, talkative, and informative.\n",
        "Answer the user's question based on the search results below:\n",
        "Search results:\n",
        "{search_results}\n",
        "User question:\n",
        "{question}\n",
        "Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "# Generate the final response using the model\n",
        "response = model.generate_content(\n",
        "    response_template.format(search_results=search_results, question=f\"Give me a summary of the functionalities and usage purpose of the following website '{website_url}' in more than 200 words\")\n",
        ")\n",
        "\n",
        "# Output the final response\n",
        "website_info = response.text\n",
        "\n",
        "# Define the template for generating questions\n",
        "questions_template = \"\"\"\n",
        "Generate 10 questions based on the following information about a website.\n",
        "Ensure each question is no more than 70 characters long.\n",
        "Information:\n",
        "{website_info}\n",
        "Questions:\n",
        "\"\"\".strip()\n",
        "\n",
        "# Generate questions based on the website information\n",
        "matching_response = model.generate_content(\n",
        "    questions_template.format(website_info=website_info)\n",
        ")\n",
        "\n",
        "# Output the generated questions\n",
        "matching_text = matching_response.text\n",
        "\n",
        "# Save the questions to a JSON file\n",
        "matching_list = matching_text.split('\\n')\n",
        "matching_data = {\"questions\": matching_list}\n",
        "\n",
        "\n",
        "with open('matching_questions.json', 'w') as json_file:\n",
        "    json.dump(matching_data, json_file, indent=4)\n",
        "\n",
        "print(\"Questions saved to 'matching_questions.json'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "rvThaDnYVzMv",
        "outputId": "eec19f4c-31b3-4d77-e1eb-67e232b26714"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions saved to 'matching_questions.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Load the generated questions from the JSON files\n",
        "with open('generated_questions.json', 'r') as file:\n",
        "    data_1 = json.load(file)\n",
        "    questions_1 = data_1['questions']\n",
        "\n",
        "with open('matching_questions.json', 'r') as file:\n",
        "    data_2 = json.load(file)\n",
        "    questions_2 = data_2['questions']\n",
        "\n",
        "# Create a TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the questions from both results\n",
        "tfidf_matrix_1 = vectorizer.fit_transform(questions_1)\n",
        "tfidf_matrix_2 = vectorizer.transform(questions_2)\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix_1, tfidf_matrix_2)\n",
        "\n",
        "# Find the best matches for each question in the first set\n",
        "matches = np.argmax(similarity_matrix, axis=1)\n",
        "match_scores = similarity_matrix[np.arange(len(questions_1)), matches]\n",
        "\n",
        "# Define thresholds\n",
        "match_threshold = 0.5  # Adjust as needed\n",
        "\n",
        "# Calculate direct match percentage\n",
        "num_matches = np.sum(match_scores > match_threshold)\n",
        "direct_match_percentage = num_matches / len(questions_1) * 100\n",
        "\n",
        "# Calculate the similarity of unmatched questions\n",
        "unmatched_scores = np.full(len(questions_1), 0.0)  # Initialize array to store similarity scores for unmatched questions\n",
        "\n",
        "for i in range(len(questions_1)):\n",
        "    matched_index = matches[i]\n",
        "    # Calculate similarity of this question to all other questions in the second set\n",
        "    similarity_to_others = similarity_matrix[i]\n",
        "    # Exclude the similarity to the matched question\n",
        "    similarity_to_others[matched_index] = 0\n",
        "    # Get the maximum similarity score from the remaining questions\n",
        "    unmatched_scores[i] = np.max(similarity_to_others)\n",
        "\n",
        "# Calculate the average similarity of unmatched questions\n",
        "average_unmatched_similarity = np.mean(unmatched_scores)*100\n",
        "match_accuracy = (num_matches*100)/10\n",
        "\n",
        "# Refine the accuracy score based on unmatched question similarity\n",
        "# Ensure the refinement factor scales appropriately to reflect the closeness impact\n",
        "direct_match_weight = 0.7  # Weight for direct matches\n",
        "closeness_weight = 0.3     # Weight for closeness of unmatched questions\n",
        "\n",
        "# Adjust the maximum possible score to ensure the refined score is reasonable\n",
        "max_possible_score = 100\n",
        "refined_accuracy_score = (direct_match_percentage * direct_match_weight) + (average_unmatched_similarity * 100 * closeness_weight)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"Number of direct matches:\", num_matches)\n",
        "print(\"Direct Match Accuracy (%):\", match_accuracy)\n",
        "print(\"Average similarity of unmatched questions:\", average_unmatched_similarity)\n",
        "print(\"Refined Accuracy Score (%):\",match_accuracy + average_unmatched_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-ydhoSgi1Tx",
        "outputId": "69b85c0b-2816-4eb7-8786-af87ecb1e789"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of direct matches: 6\n",
            "Direct Match Accuracy (%): 60.0\n",
            "Average similarity of unmatched questions: 23.879934077561785\n",
            "Refined Accuracy Score (%): 83.87993407756178\n"
          ]
        }
      ]
    }
  ]
}